{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# LoRA Implementation\n",
    "\n"
   ],
   "id": "c5a62e6bdd11c536"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Import Libraries\n",
    "First, we need to import the necessary libraries for our implementation. These include PyTorch for building and training the model, the Transformers library for using pre-trained BERT, and the Datasets library for loading the SST-2 dataset."
   ],
   "id": "874509388b1c16d8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import BertModel, BertTokenizer, AdamW, get_linear_schedule_with_warmup\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n"
   ],
   "id": "d8dafe0ca57a45d8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Define LoRA Layer and Model\n",
    "Here, we define the LoRALayer class, which introduces low-rank adaptation by adding trainable low-rank matrices to the existing weights. We also define the LoRABertModel class that integrates the LoRALayer into a pre-trained BERT model and adds a classification head.\n"
   ],
   "id": "8dd8b14327884439"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class LoRALayer(nn.Module):\n",
    "    def __init__(self, in_features, out_features, rank):\n",
    "        super(LoRALayer, self).__init__()\n",
    "        self.rank = rank\n",
    "        self.lora_a = nn.Parameter(torch.randn(in_features, rank))\n",
    "        self.lora_b = nn.Parameter(torch.randn(rank, out_features))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + (x @ self.lora_a @ self.lora_b)\n",
    "\n",
    "class LoRABertModel(nn.Module):\n",
    "    def __init__(self, model_name, rank):\n",
    "        super(LoRABertModel, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained(model_name)\n",
    "        self.lora = LoRALayer(self.bert.config.hidden_size, self.bert.config.hidden_size, rank)\n",
    "        self.classifier = nn.Linear(self.bert.config.hidden_size, 2)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids, attention_mask=attention_mask)\n",
    "        last_hidden_state = outputs.last_hidden_state\n",
    "        lora_output = self.lora(last_hidden_state[:, 0, :])  # Use [CLS] token representation\n",
    "        logits = self.classifier(lora_output)\n",
    "        return logits\n"
   ],
   "id": "3a3332a92c470b9e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Initialize Tokenizer and Model\n",
    "We initialize the BERT tokenizer and the LoRABertModel. The tokenizer will preprocess the text data, and the model will be used for training and evaluation.\n"
   ],
   "id": "e82e5e43df5a7ef4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = LoRABertModel(\"bert-base-uncased\", rank=4)\n"
   ],
   "id": "c6a1f1ab5dd375a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Load and Prepare Dataset\n",
    "We load the SST-2 dataset from the GLUE benchmark using the Datasets library. The dataset is then tokenized and formatted for use in PyTorch DataLoader.\n"
   ],
   "id": "d1611f29be98789c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "total_steps = len(train_dataloader) * 3  # 3 epochs\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "for epoch in range(3):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in train_dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "        \n",
    "        outputs = model(input_ids, attention_mask)\n",
    "        loss = nn.CrossEntropyLoss()(outputs, labels)\n",
    "        loss.backward()\n",
    "        total_loss += loss.item()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "    \n",
    "    avg_train_loss = total_loss / len(train_dataloader)\n",
    "    print(f\"Epoch {epoch + 1}, Loss: {avg_train_loss}\")\n"
   ],
   "id": "c46f5f5cf4e33ec8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Training Loop\n",
    "We define the training loop for the model. This includes setting up the optimizer, learning rate scheduler, and training the model for a specified number of epochs. The training loss is printed at the end of each epoch.\n"
   ],
   "id": "8c84babc42715a78"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "total_steps = len(train_dataloader) * 3  # 3 epochs\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "for epoch in range(3):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in train_dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "        \n",
    "        outputs = model(input_ids, attention_mask)\n",
    "        loss = nn.CrossEntropyLoss()(outputs, labels)\n",
    "        loss.backward()\n",
    "        total_loss += loss.item()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "    \n",
    "    avg_train_loss = total_loss / len(train_dataloader)\n",
    "    print(f\"Epoch {epoch + 1}, Loss: {avg_train_loss}\")\n"
   ],
   "id": "5ad8b0563b5d1b1f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Validation Loop\n",
    "We evaluate the model on the validation set to measure its accuracy. The number of correct predictions is compared to the total number of samples to calculate the accuracy.\n"
   ],
   "id": "53169afb42762f6f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for batch in val_dataloader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "        \n",
    "        outputs = model(input_ids, attention_mask)\n",
    "        predictions = torch.argmax(outputs, dim=1)\n",
    "        correct += (predictions == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "accuracy = correct / total\n",
    "print(f\"Validation Accuracy: {accuracy}\")\n"
   ],
   "id": "8b76c12bbb930dcf"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
