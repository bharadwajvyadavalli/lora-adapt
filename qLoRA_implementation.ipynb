{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## q-LoRA Implementation",
   "id": "ddcfd82dee7e8119"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Import Libraries\n",
    "First, we need to import the necessary libraries for our implementation. These include PyTorch for building and training the model, the Transformers library for using pre-trained BERT, and the Datasets library for loading the SST-2 dataset.\n"
   ],
   "id": "76f00d07af3c80e1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import BertModel, BertTokenizer, AdamW, get_linear_schedule_with_warmup\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n"
   ],
   "id": "1f989c68edbf220b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Define Quantized LoRA Layer and Model\n",
    "Here, we define the QuantizedLoRALayer class, which introduces low-rank adaptation with quantization by adding trainable quantized low-rank matrices to the existing weights. We also define the qLoRABertModel class that integrates the QuantizedLoRALayer into a pre-trained BERT model and adds a classification head.\n"
   ],
   "id": "eac2220127bfa3ce"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class QuantizedLoRALayer(nn.Module):\n",
    "    def __init__(self, in_features, out_features, rank):\n",
    "        super(QuantizedLoRALayer, self).__init__()\n",
    "        self.rank = rank\n",
    "        self.lora_a = nn.Parameter(torch.randn(in_features, rank))\n",
    "        self.lora_b = nn.Parameter(torch.randn(rank, out_features))\n",
    "\n",
    "    def forward(self, x):\n",
    "        quantized_a = torch.quantize_per_tensor(self.lora_a, scale=0.1, zero_point=0, dtype=torch.qint8)\n",
    "        quantized_b = torch.quantize_per_tensor(self.lora_b, scale=0.1, zero_point=0, dtype=torch.qint8)\n",
    "        return x + (x @ quantized_a.dequantize() @ quantized_b.dequantize())\n",
    "\n",
    "class qLoRABertModel(nn.Module):\n",
    "    def __init__(self, model_name, rank):\n",
    "        super(qLoRABertModel, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained(model_name)\n",
    "        self.lora = QuantizedLoRALayer(self.bert.config.hidden_size, self.bert.config.hidden_size, rank)\n",
    "        self.classifier = nn.Linear(self.bert.config.hidden_size, 2)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids, attention_mask=attention_mask)\n",
    "        last_hidden_state = outputs.last_hidden_state\n",
    "        lora_output = self.lora(last_hidden_state[:, 0, :])  # Use [CLS] token representation\n",
    "        logits = self.classifier(lora_output)\n",
    "        return logits\n"
   ],
   "id": "1c85b26ddf7a2a70"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Initialize Tokenizer and Model\n",
    "We initialize the BERT tokenizer and the qLoRABertModel. The tokenizer will preprocess the text data, and the model will be used for training and evaluation.\n"
   ],
   "id": "f6fed6edf921a10f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = qLoRABertModel(\"bert-base-uncased\", rank=4)\n"
   ],
   "id": "c63811b89b56e44"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Load and Prepare Dataset\n",
    "We load the SST-2 dataset from the GLUE benchmark using the Datasets library. The dataset is then tokenized and formatted for use in PyTorch DataLoader.\n"
   ],
   "id": "b3ec5ced9966ba78"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "dataset = load_dataset('glue', 'sst2')\n",
    "train_dataset = dataset['train']\n",
    "val_dataset = dataset['validation']\n",
    "\n",
    "def encode(examples):\n",
    "    return tokenizer(examples['sentence'], truncation=True, padding='max_length', max_length=128)\n",
    "\n",
    "train_dataset = train_dataset.map(encode, batched=True)\n",
    "val_dataset = val_dataset.map(encode, batched=True)\n",
    "\n",
    "train_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "val_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, sampler=RandomSampler(train_dataset), batch_size=16)\n",
    "val_dataloader = DataLoader(val_dataset, sampler=SequentialSampler(val_dataset), batch_size=16)\n"
   ],
   "id": "1ff767fb2f01ab10"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Training Loop\n",
    "We define the training loop for the model. This includes setting up the optimizer, learning rate scheduler, and training the model for a specified number of epochs. The training loss is printed at the end of each epoch.\n"
   ],
   "id": "b593b45a76c18b0b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "total_steps = len(train_dataloader) * 3  # 3 epochs\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "for epoch in range(3):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in train_dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "        \n",
    "        outputs = model(input_ids, attention_mask)\n",
    "        loss = nn.CrossEntropyLoss()(outputs, labels)\n",
    "        loss.backward()\n",
    "        total_loss += loss.item()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "    \n",
    "    avg_train_loss = total_loss / len(train_dataloader)\n",
    "    print(f\"Epoch {epoch + 1}, Loss: {avg_train_loss}\")\n"
   ],
   "id": "780502bdb01ac7ae"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Validation Loop\n",
    "We evaluate the model on the validation set to measure its accuracy. The number of correct predictions is compared to the total number of samples to calculate the accuracy.\n"
   ],
   "id": "c42ad2ae5e764fb7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for batch in val_dataloader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "        \n",
    "        outputs = model(input_ids, attention_mask)\n",
    "        predictions = torch.argmax(outputs, dim=1)\n",
    "        correct += (predictions == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "accuracy = correct / total\n",
    "print(f\"Validation Accuracy: {accuracy}\")\n"
   ],
   "id": "1701ceb7daa8aba5"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
